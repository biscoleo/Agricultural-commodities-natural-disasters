{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC \n",
    "import pyautogui\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import TimeoutException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping and Pickling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I really wanted to incorporate some web scraping into this project. After comparing several datasets, I found a dataset from the FAO for commodities that looked great and one from EM-DAT for natural disasters that seemed comprehensive. Unfortunately, to access EM-DAT, you need a password and entering that from a script and then scraping felt sketchy. The FAO dataset is hugely comprehensive and would not load a view on the webpage, so I opted to just download it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FAOSTAT - Commodities Data: \n",
    "Globally and by country\n",
    "https://www.fao.org/faostat/en/#rankings/countries_by_commodity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globally:\n",
    "quick_pickle = pd.read_csv('data/FAOSTAT_data_2.csv')\n",
    "quick_pickle.to_pickle('quick_pickle.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ly/bxd94jrj5f1ffy3zk8_yklrh0000gn/T/ipykernel_3598/2437603376.py:2: DtypeWarning: Columns (11,14,17,20,23,26,29,32,35,38,41,44,47,50,53,56,59,62,65,68,71,74,77,80,83,86,89,92,95,98,101,104,107,110,113,116,119,122,125,128,131,134,137,140,143,146,149,152,155,158,161,164,167,170,173,176,179,182,185,188,191,194) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  bread_and_butter = pd.read_csv('data/Production_Crops_Livestock_E_All_Data.csv',encoding='ISO-8859-1')\n"
     ]
    }
   ],
   "source": [
    "# By Country:\n",
    "bread_and_butter = pd.read_csv('data/Production_Crops_Livestock_E_All_Data.csv',encoding='ISO-8859-1')\n",
    "bread_and_butter.to_pickle('bread_and_butter.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trade Indices:\n",
    "#https://www.fao.org/faostat/en/#data/TI\n",
    "trade = pd.read_csv('data/Trade_Indices_E_All_Data.csv',encoding='ISO-8859-1')\n",
    "trade.to_pickle('trade.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EM-DAT dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "nat_dillsaster = pd.read_excel('data/public_emdat_all1.xlsx',engine='openpyxl')\n",
    "nat_dillsaster.to_pickle('nat_dillsaster.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I did find a GDP dataset I wanted to incoporate into my analysis. I also ran into an issue on this page. It is a scrolling table and 'Inspect Elements' doesn't show the html for the entire table. You have to slide the scroll bar to load the rest of the html, presumably with selenium. This seemed unnecessarily complicated, so I opted for a workaround that is only maybe considered successful scraping. \n",
    "\n",
    "I used selenium to print the whole table. I then tried various ways to unlock my accessibility features to hit 'esc' to close the Print Dialogue Box, because below it is a super simple and complete table of all the GDP data I need. (I know we said scraping comes to a stop if there is a popup, but my curiosity clearly needed to see for itself.) The user must hit cancel on the print dalogue box, and then the code switches to that other webpage(since the print preview is generated by Chrome) and scrapes it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chromedriver_path = '/Users/annaleoni/Desktop/Final/pkl_jar/chromedriver-mac-arm64/chromedriver'\n",
    "# service = Service(chromedriver_path)\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service)\n",
    "# GO TO WEBSITE\n",
    "driver.get('https://wits.worldbank.org/CountryProfile/en/country/by-country/startyear/ltst/endyear/ltst/indicator/NY-GDP-MKTP-CD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current URL: https://wits.worldbank.org/CountryProfile/en/country/by-country/startyear/ltst/endyear/ltst/indicator/NY-GDP-MKTP-CD\n",
      "Page title: Indicators by Trading Partner - Print\n",
      "<html><head>\n",
      "<meta charset=\"utf-8\">\n",
      "<title>Indicators by Trading Partner - Print </title>\n",
      "<style>\n",
      "bo\n",
      "['Country Name', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', 'TooltipID']  <-- Headers\n",
      "data [['United States', '5,236,438,000,000.00', '5,641,580,000,000.00', '5,963,144,000,000.00', '6,158,129,000,000.00', '6,520,327,000,000.00', '6,858,559,000,000.00', '7,287,236,000,000.00', '7,639,749,000,000.00', '8,073,122,000,000.00', '8,577,554,457,000.00', '9,062,818,202,000.00', '9,631,174,489,000.00', '10,250,947,997,000.00', '10,581,929,774,000.00', '10,929,112,955,000.00', '11,456,442,041,000.00', '12,217,193,198,000.00', '13,039,199,193,000.00', '13,815,586,948,000.00', '14,474,226,905,000.00', '14,769,857,911,000.00', '14,478,064,934,000.00', '15,048,964,444,000.00', '15,599,728,123,000.00', '16,253,972,230,000.00', '16,843,190,993,000.00', '17,550,680,174,000.00', '18,206,020,741,000.00', '18,695,110,842,000.00', '19,477,336,549,000.00', '20,533,057,312,000.00', '21,380,976,119,000.00', '21,060,473,613,000.00', '23,315,080,560,000.00', '53.00'], ['Japan', '3,071,683,812,149.66', '3,054,913,797,217.03', '3,132,817,652,848.04', '3,584,420,964,070.07', '3,908,808,434,705.26', '4,454,144,444,407.90', '4,998,797,262,443.29', '5,545,564,892,181.15', '4,923,393,495,138.77', '4,492,449,998,031.88', '4,098,362,688,659.34', '4,635,982,020,564.97', '4,968,359,152,795.66', '4,374,709,984,220.01', '4,182,845,406,488.62', '4,519,563,042,183.73', '4,893,116,005,656.56', '4,831,466,523,975.98', '4,601,662,661,030.07', '4,579,749,785,984.82', '5,106,679,413,137.57', '5,289,493,734,900.39', '5,759,071,769,013.11', '6,233,147,172,341.35', '6,272,362,996,105.03', '5,212,328,181,166.18', '4,896,994,405,353.29', '4,444,930,651,964.18', '5,003,677,627,544.24', '4,930,837,369,151.42', '5,040,880,939,324.86', '5,117,993,853,016.51', '5,048,789,595,589.43', '5,005,536,736,792.29', '53.00'], ['Germany', '1,401,233,225,303.49', '1,398,967,436,804.33', '1,771,671,206,875.68', '1,868,945,197,407.19', '2,131,571,696,931.75', '2,071,323,790,370.28', '2,205,074,123,177.05', '2,585,792,275,146.72', '2,497,244,606,186.64', '2,211,989,623,279.95', '2,238,990,774,702.68', '2,194,945,278,872.59', '1,947,981,991,011.77', '1,945,790,973,803.15', '2,078,484,517,474.51', '2,501,640,388,482.35', '2,814,353,869,359.08', '2,846,864,211,175.10', '2,994,703,642,023.53', '3,425,578,382,921.58', '3,745,264,093,617.19', '3,411,261,212,652.34', '3,399,667,820,000.09', '3,749,314,991,050.62', '3,527,143,188,785.16', '3,733,804,649,549.06', '3,889,093,051,023.45', '3,357,585,719,351.56', '3,469,853,463,945.63', '3,690,849,152,517.69', '3,974,443,355,019.53', '3,888,226,035,921.49', '3,889,668,895,299.56', '4,259,934,911,821.64', '53.00']]\n",
      "shape:  (193, 36)\n"
     ]
    }
   ],
   "source": [
    "# Navigate to print button and select all:\n",
    "try:\n",
    "    print_button_1 = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.ID, \"DataPrint\")))\n",
    "    print_button_1.click()\n",
    "    time.sleep(2)\n",
    "\n",
    "    print_all_radio_button = WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.ID, \"All\")))\n",
    "    print_all_radio_button.click()\n",
    "    time.sleep(3)\n",
    "\n",
    "    print_button_2 = WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.ID, \"DataPrintBtn\")))\n",
    "    print_button_2.click()\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "    # pyautogui.press('esc')\n",
    "    # alternative:\n",
    "    actions = ActionChains(driver)\n",
    "    actions.send_keys(Keys.ESCAPE).perform()\n",
    "    time.sleep(5)\n",
    "\n",
    "    driver.switch_to.window(driver.window_handles[-1])\n",
    "    print(\"Current URL:\", driver.current_url)\n",
    "    print(\"Page title:\", driver.title)\n",
    "\n",
    "    # now i'm at the print table layout:\n",
    "    html_content = driver.page_source\n",
    "    print(html_content[:100])\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    table = soup.find('table')\n",
    "\n",
    "    if table is None:\n",
    "        print(\"No table found in HTML content\")\n",
    "    else:\n",
    "\n",
    "        #get headers\n",
    "        headers = ['Country Name'] + [th.text.strip() for th in table.find_all('th')[1:]]\n",
    "        print(headers, \" <-- Headers\")\n",
    "\n",
    "        data=[]\n",
    "        for row in table.find_all('tr')[1:]:\n",
    "            cols = row.find_all('td')\n",
    "            if cols:\n",
    "                country_name = cols[0].text.strip()\n",
    "                gdp_vals = [col.text.strip() for col in cols[1:]]\n",
    "                data.append([country_name] + gdp_vals)\n",
    "        print(\"data\", data[:3])\n",
    "\n",
    "        # dataframe it\n",
    "        gdp_df = pd.DataFrame(data, columns=headers)\n",
    "        print(\"shape: \", gdp_df.shape)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_df.head()\n",
    "gdp_df.to_pickle('GarlicDillPickle.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP3006_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
